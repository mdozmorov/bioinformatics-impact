---
bibliography: impact.bib
csl: styles/frontiers-in-bioengineering-and-biotechnology.csl
output:
  word_document:
    reference_docx: styles/Frontiers_Template.docx
  pdf_document: default
  html_document: default
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# Set up the environment
library(knitr)
opts_chunk$set(cache.path='cache/', fig.path='img/', cache=T, tidy=T, fig.keep='high', echo=F, dpi=100, warnings=F, message=F, comment=NA, warning=F, results='as.is', fig.width = 10, fig.height = 6, eval = FALSE) #out.width=700, 
library(pander)
panderOptions('table.split.table', Inf)
set.seed(1)
library(dplyr)
options(stringsAsFactors = FALSE)
```

## GitHub statistics as a measure of the impact of open-source bioinformatics software

Mikhail G. Dozmorov^1\*^

^1^ - Department of Biostatistics, Virginia Commonwealth University, 830 East Main Street, Richmond, Virginia, USA.

^\*^ **Correspondence:**   
Mikhail Dozmorov  
mikhail.dozmorov@vcuhealth.org

<!-- Running title: Impact of bioinformatics software -->

**Keywords: bioinformatics, software, impact factor, altmetrics, github**

## Abstract

Modern research is increasingly data-driven and reliant on bioinformatics software. Publication is a common way of introducing new software, but not all bioinformatics tools get published. Giving there are competing tools, it is important not merely to find the appropriate software, but have a metric for judging its usefulness. Journal's impact factor has been shown to be a poor predictor of software popularity; consequently, focusing on publications in high-impact journals limits user's choices in finding useful bioinformatics tools. Free and open source software repositories on popular code sharing platforms such as GitHub provide another venue to follow the latest bioinformatics trends. The open source component of GitHub allows users to bookmark and copy repositories that are most useful to them. This Perspective aims to demonstrate the utility of GitHub "stars," "watchers," and "forks" (GitHub statistics) as a measure of software impact. We compiled lists of impactful bioinformatics software and analyzed commonly used impact metrics and GitHub statistics of 50 genomics-oriented bioinformatics tools. We present examples of community-selected best bioinformatics resources and show that GitHub statistics are distinct from the journal's impact factor (JIF), citation counts, and alternative metrics (Altmetrics, CiteScore) in capturing the level of community attention. We suggest the use of GitHub statistics as an unbiased measure of the usability of bioinformatics software complementing the traditional impact metrics.

# Introduction

It is currently undeniable that bioinformatics tools and databases represent a highly impactful part of modern research [@Wren:2016aa]. Many journals focus exclusively on publishing software tools and databases. Some of the most famous examples include application notes published in _Bioinformatics_, database and web-server issues published by _Nucleic Acids Research_, software articles published in _Frontiers Bioinformatics and Computational Biology_, _PLOS Computational Biology_, _BMC Bioinformatics_. However, given the continued growth of bioinformatics publications [@Wren:2016aa] (Supplementary Figure S1), it is getting increasingly difficult to find software that will be useful in real-life applications. Recently, a term "software crisis" was coined to illustrate the problem of finding useful software [@Mangul452532].

Finding useful bioinformatics software is further hindered by publication lag. It often takes more than a year from the time of presubmission inquiry, potential resubmission and the peer-review period to the accepted publication. Such delays inevitably diminish the potential impact of published software. Non-peer-reviewed preprint publishing (arXiv, biorXiv, PeerJ, AsapBio) aims to eliminate publication lag. However, the number of preprints grows nearly ten times faster than the number of peer-reviewed publications[^1][^2] <!--[@preprint_growth:2018aa; @prepubmed:2018aa]-->, further complicating finding useful software. 

[^1]: https://www.crossref.org/blog/preprints-growth-rate-ten-times-higher-than-journal-articles/
[^2]: http://www.prepubmed.org/monthly_stats/?Subject=Bioinformatics

Reviews of bioinformatics resources can help orient a scientist in the wealth of published tools and databases. Such reviews are typically written about bioinformatics software published in high-impact journals while leaving preprints and unpublished software largely out of scope. Furthermore, reviews may be limited by the experience of the authors, as well as by a bias to review software published in high-impact journals. Thus, while helpful in orienting a novice in the topic, reviews may overlook useful bioinformatics resources. 

Although the peer-review process helps to publish high-quality bioinformatics software, it is unknown at the time of publication which tools and databases will be embraced by the scientific community and which will be forgotten [@Wren:2008aa]. In fact, a study based on text mining found that over 70% of published bioinformatics software resources are never reused [@Duck2016]. A recent analysis of the usability of bioinformatics software confirmed these observations by highlighting issues with software accessibility and installation [@Mangul452532]. Notably, a journal's impact factor, calculated as the average number of citations received in a calendar year by the total number of articles and reviews published in that journal in the preceding two years<!--published by the Web of Science/Journal Citation Reports--> (JIF) is not a good predictor of software popularity [@Wren:2016aa; @seglen1997impact], making it hard to predict whether a bioinformatics tool or a database published in a high-impact journal will be useful in real-life applications.
<!--
**Table 1. Definition of commonly used impact metrics.**

| Metric                      | Definition                                                                                                                                                                          | Reference              |
|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
| Journal Impact Factor (JIF) | The average number of citations received in a calendar year by articles and reviews published in that journal in the preceding two years                                            | [@garfield2006history] |
| CiteScore                   | The average number of citations received in a calendar year by all items published in that journal and conference proceedings in the preceding three years                          | [@da2017citescore]     |
| Altmetrics                  | Aggregated measure of user's attention from social media networks, online discussions, etc.                                                                                          | [@priem2010altmetrics] |
-->

# Limitations of alternative metrics to measure the impact of bioinformatics software

Alternative metrics have been proposed to alleviate the shortcomings of JIF or the lack of it in preprint publishing. CiteScore, a metric developed by Scopus includes more document types and citation sources, and uses the 3-year time window to calculate the ratio of citations over the total number of citable items, has been proposed as a consistent alternative to JIF [@da2017citescore]. <!--Eigenfactor represents the probability of reading a specific journal in the entire collection of journals, hence measuring the greater influence of the journal [@fersht2009most]. -->Article-level metrics, or Altmetrics, is currently the most widely used alternative to measure the impact of scholarly material, including preprints [@priem2010altmetrics; @shema2014blog]. In addition to academic citations, this metric aggregates mentions in social media networks, such as Twitter, online discussions and recommendations. Although in principle Altmetrics can be applied to any research output that has a digital object identifier (DOI), including datasets, code and software [@Piwowar:2013aa], its use for measuring the impact of bioinformatics software is less common. <!--Specialized metrics for software impact have been proposed, such as the now-defunct Depsy project [@Singh-Chawla:2016aa]. -->Furthermore, Altmetrics may still be biased by high impact factor (hence, greater exposure and discussion) [@adie2013gaming], and overlook the practical usability of software. The usefulness of these alternative metrics on measuring the impact of bioinformatics software remains unknown.

# Community-guided selection of bioinformatics resources

<!--Currently, the majority of bioinformatics tools is being developed as free and open-source software [@perez2014developing]. -->An increasing number of bioinformaticians choose to develop their tools on popular code sharing web services, such as GitHub [@Wilson:2017aa]. Besides code-sharing services, GitHub combines a version control system <!--@Blischak:2016aa-->[@Bryant:2017aa] with features found in popular social network sites such as Facebook and Twitter [@Lima:2014aa]<!--[@Dabbish2012]-->. Users may try the tools and bookmark the most practically useful ones by "starring," "watching," and/or "forking" them<!-- [@Sheoran2014]-->. "Starring" a repository is similar to bookmarking it as a favorite, while "watching" is a more advanced feature allowing a user to receive all, or selected, updates about a repository. "Forking" further advances user's involvement by creating a copy of a forked repository under the user's account, allowing him/her to offer code enhancements by creating pull requests. GitHub creates a natural ecosystem for software development where the amount of community attention to a repository is directly proportional to its popularity [@Hu2016]. We expect the number of stars, watchers, and forks ("GitHub statistics") to reflect some evidence of the practical utility of the software and suggest they should be used to inform selection of the most useful resources.

# Lists of community-selected software as reviews of practical utility

Although using GitHub statistics as a guide for selecting the most popular software<!--[@gitstar-ranking:2018aa]-->, including bioinformatics tools, has been suggested[^3] [@Russell2018; @Hu2016], it does not alleviate the problem of finding the right field-specific resources among a large number of bioinformatics repositories[^4]<!-- [@too_many_tools:2013aa]-->. The abundance of GitHub repositories gave rise to field-specific collections of the most useful resources (tools, databases, papers, books, and videos), frequently referred to as "awesome" lists (Table 1, Supplementary Table S1). They are assembled by inspired individuals who empirically try them and bookmarks the most valuable repositories [@Marlow2013]<!-- [@Sheoran2014]-->. These collections of links and notes are themselves published on GitHub and starred by the community. The collections may themselves be assembled into field-specific "awesome" lists of lists (Supplementary Table S2). Being analogous to bookmarks freely accessible on the web, they do not require any programming skills to be used. These collections may be compared with field-specific reviews peer-reviewed by the community and may be used to quickly prioritize practically useful resources.

[^3]: https://gitstar-ranking.com/
[^4]: https://www.researchgate.net/post/Is_there_too_many_bioinformatics_tools2
<!--
+-------+
| ![](figures/Figure_impact_PCA.png)      |
| **Figure 1. Principal component analysis of bioinformatics impact measures, colored by metric type.**     |
+-------+
-->

# Community attention as a distinct and universal measure of software impact

To better understand the relationship between community attention-based and traditional impact metrics, we compared GitHub statistics, JIF, CiteScore, Altmetrics, citation count, and software age of 50 popular genomics-oriented bioinformatics tools published in peer-review journals, developed on GitHub, and starred 50 times or more (Supplementary Table S3, Methods[^5]<!--, reproducible cote at [@bioinformatics-impact:2018aa]-->). Principal component analysis (PCA, Figure 1) and correlation analysis (Supplementary Figure S2) showed the expected correlation between similarly calculated JIF and CiteScore (Pearson Correlation Coefficient, PCC = 0.73). The software age and citation counts were also correlated (PCC = 0.60) as would be expected for older software having more chance of being cited. However, neither the software age nor citation counts were correlated with JIF (PCC = -0.23/-0.02, respectively), suggesting that citations of bioinformatics software have minimal effect on JIF. Furthermore, the correlation between JIF and Altmetrics was relatively modest (PCC = 0.49), suggesting that Altmetrics captures a different level of impact. The poor correlation among traditional impact metrics complicates their use for measuring the software impact.

Being a measure of attention of open-source software development community, GitHub statistics are expected to capture the practical usability of software that may be missed by traditional impact metrics. Indeed, GitHub statistics (counts of "stars," "watches," and "forks") were highly correlated with each other (average PCC = 0.92) but were distinct from other metrics. Neither JIF nor Altmetrics correlated with GitHub statistics (average PCC = -0.09/0.14, respectively), highlighting differences between community attention-based and traditional impact metrics. Interestingly, GitHub statistics and citation counts showed modest correlation (average PCC = 0.66), suggesting that practically useful software cited more frequently. However, the software age correlated with GitHub statistics to a much lesser extent (average PCC = 0.32), suggesting that the age of the software does not necessarily indicate its usefulness. We suggest that GitHub statistics should be used as an objective addition to JIF and other traditional impact metrics in measuring the practical utility of bioinformatics software. 

[^5]: Methods at https://github.com/mdozmorov/bioinformatics-impact

# Author contributions

MD envisioned the project, collected and analyzed the data, and wrote the manuscript.

# Acknowledgements

The author would like to thank Dr. Jonathan D. Wren and John C. Stansfield for discussions and feedback.

# Conflict of interest

The author declares that he has no conflict of interest.

# Figures

**Figure 1. Principal component analysis of bioinformatics impact measures, colored by metric type.**

<!--
**Supplemental Figure S1. Growth of publications having the term "bioinformatics" in their title/abstract.** Y-axis is the proportion of bioinformatics publications out of the total number of publications, in percent.

**Supplemental Figure S2. Correlogram of bioinformatics software impact metrics.** Each cell shows Pearson Correlation Coefficient (PCC) for the corresponding pair of metrics. Blue/Red gradient highlights low/high PCC, respectively.

**Supplemental Table S1. Select data science resources.** Metrics in all tables were assessed on 2018-10-05.

**Supplemental Table S2. Examples of lists of lists of computer science and machine learning resources.**

**Supplemental Table S3. Impact metrics of popular bioinformatics tools and resources.** Only the software that is being developed on GitHub, has over 50 stars, and published in peer-review journals was selected.
-->

# References
